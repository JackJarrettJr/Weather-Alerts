{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a39961-ef88-4c98-bdaf-32599b4e4106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# NBA Best Performance using Real-time Data Processing with Azure Databricks (and Event Hubs)\n",
    "\n",
    "This notebook demonstrates a real-time data processing workflow in Databricks using Structured Streaming to ingest data from Event Hubs. Designed a Bronze-Silver-Gold architecture to refine and transform data to find the weather conditions, with an automated email alert to notify stakeholders of the top results.\n",
    "\n",
    "- Data Sources: Streaming data from IoT devices or social media feeds. (Simulated in Event Hubs)\n",
    "- Ingestion: Azure Event Hubs for capturing real-time data.\n",
    "- Processing: Azure Databricks for stream processing using Structured Streaming.\n",
    "- Storage: Processed data stored Azure Data Lake (Delta Format).\n",
    "\n",
    "### Azure Services Required\n",
    "- Databricks Workspace\n",
    "- Azure Data Lake Storage\n",
    "- Azure Event Hub\n",
    "\n",
    "### Azure Databricks Configuration Required\n",
    "- Single Node Compute Cluster: `12.2 LTS (includes Apache Spark 3.3.2, Scala 2.12)`\n",
    "- Maven Library installed on Compute Cluster: `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.22`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "450c2393-858b-4e6f-bad7-938660ce7bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sample Data:  \n",
    "###     {\n",
    "###     \"temperature\": 40,  \n",
    "###     \"humidity\": 20,  \n",
    "###     \"windSpeed\": 10,   \n",
    "###     \"windDirection\": \"NW\",  \n",
    "###     \"precipitation\": 0,  \n",
    "###     \"conditions\": \"Partly Cloudy\"  \n",
    "### }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1a287c0-1c5e-4649-ad56-a7b03707a663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376948f-980e-4887-a319-45a525172d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b5cc9f6-21cd-4201-a324-e97122c8203f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The code block below creates the catalog and schemas for our solution. \n",
    "\n",
    "The approach utilises a multi-hop data storage architecture (medallion), consisting of bronze, silver, and gold schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49397d51-5412-4079-9159-06307954a7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.bronze;\")\n",
    "except:\n",
    "    print('check if bronze schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metastore.silver;\")\n",
    "except:\n",
    "    print('check if silver schema already exists')\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS hive_metadata.gold;\")\n",
    "except:\n",
    "    print('check if gold schema already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b61da0-3b55-4941-8436-2411646fa4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8806de51-7d1e-4dcc-8981-27f5a3447f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Set up Azure Event hubs connection string.\n",
    "* Defining JSON Schema\n",
    "* Reading and writing the stream to Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3397f5f1-841b-4911-9099-8b99b92ab7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "# Replace with your Event Hub namespace, name, and key\n",
    "connectionString = \"Endpoint=sb://jack-namespace-demo.servicebus.windows.net/;SharedAccessKeyName=databricks;SharedAccessKey=########################################;EntityPath=eh_demo\"\n",
    "\n",
    "\n",
    "ehConf = {\n",
    "  \"eventhubs.connectionString\": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "}\n",
    "\n",
    "# Define JSON schema\n",
    "json_schema = StructType([\n",
    "    StructField(\"temperature\", IntegerType()),\n",
    "    StructField(\"humidity\", IntegerType()),\n",
    "    StructField(\"windSpeed\", IntegerType()),\n",
    "    StructField(\"windDirection\", StringType()),\n",
    "    StructField(\"precipitation\", IntegerType()),\n",
    "    StructField(\"conditions\", StringType())\n",
    "])\n",
    "\n",
    "# Read raw streaming data from Event Hubs\n",
    "# No JSON parsing in Bronze layer, store raw data\n",
    "\n",
    "df_bronze = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load()\n",
    "\n",
    "# Write to Bronze Table\n",
    "df_bronze.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/bronze/weather\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .toTable(\"hive_metastore.bronze.weather\")\n",
    "\n",
    "df_bronze.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b28ff3-52b8-4a3f-ad67-758854096a2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking whether data is stored in Bronze Weather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d478fc71-1803-46bc-9abb-a0c522cb586d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM hive_metastore.bronze.weather;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbee364e-6b7e-420f-8b98-8a12d663d880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd659c9-65fc-4112-be6e-06f63196311f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Reading the stream from the bronze \n",
    "* Categorizing weather conditions into \n",
    "   - Heatwave, Storm, Thunderstorm\n",
    "* Writing the transformed stream into silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18db15d2-f3d1-4dd7-b5aa-ef421208b46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Read Bronze Data and Parse JSON in Silver Layer\n",
    "df_silver = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(\"hive_metastore.bronze.weather\") \\\n",
    "    .withColumn(\"body\", col(\"body\").cast(\"string\")) \\\n",
    "    .withColumn(\"body\", from_json(col(\"body\"), json_schema)) \\\n",
    "    .withColumn(\"eventId\", expr(\"uuid()\")) \\\n",
    "    .select(\"eventId\", \"body.*\", col(\"enqueuedTime\").alias(\"timestamp\")) \\\n",
    "    .dropDuplicates([\"eventId\"])\n",
    "\n",
    "# Categorizing weather conditions based on the data\n",
    "df_silver = df_silver \\\n",
    "    .withColumn(\"wind_category\", \n",
    "                when(col(\"windSpeed\") < 5, \"Low\")\n",
    "                .when(col(\"windSpeed\") < 15, \"Moderate\")\n",
    "                .otherwise(\"High\")) \\\n",
    "    .withColumn(\"precipitation_category\",\n",
    "                when(col(\"precipitation\") == 0, \"None\")\n",
    "                .when(col(\"precipitation\") < 5, \"Light\")\n",
    "                .when(col(\"precipitation\") < 20, \"Moderate\")\n",
    "                .otherwise(\"Heavy\")) \\\n",
    "    .withColumn(\"anomaly_type\", \n",
    "                when(col(\"temperature\") > 45, \"Heatwave\")\n",
    "                .when(col(\"windSpeed\") > 100, \"Storm\")\n",
    "                .when((col(\"humidity\") > 90) & (col(\"precipitation\") > 10), \"Thunderstorm\")\n",
    "                .otherwise(\"Normal\"))\n",
    "\n",
    "\n",
    "# Write to Silver Table\n",
    "df_silver.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/silver/weather\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .toTable(\"hive_metastore.silver.weather\")\n",
    "\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1664e9d-fe95-40d0-a842-1b0e8858cfca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking whether data is stored in Silver Weather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e52f8ffe-dc21-4403-9dd0-3b414bc77a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM hive_metastore.silver.weather;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4078a144-9652-490b-adf3-348ad8780e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c07779-5a57-4f70-8c90-34554f8a82b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Reading, aggregating and writing the stream from the silver to the gold layer\n",
    "* 5 minute Sliding Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4b337b-6bb7-490d-b1d3-2e8b83456661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Aggregating Stream: Read from 'streaming.silver.weather', apply watermarking and windowing, and calculate average weather metrics\n",
    "# **Sliding Window Aggregations (5-minute window, 5-minute slide)**\n",
    "df_gold = df_silver \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\")) \\\n",
    "    .agg(\n",
    "        avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "        avg(\"windSpeed\").alias(\"avg_windSpeed\"),\n",
    "        avg(\"precipitation\").alias(\"avg_precipitation\"),\n",
    "        sum(when(col(\"anomaly_type\") != \"Normal\", 1).otherwise(0)).alias(\"anomaly_count\")\n",
    "  \n",
    "    ) \\\n",
    "    .selectExpr(\"window.start as start_time\", \"window.end as end_time\", \"*\")\n",
    "\n",
    "# Write to Gold Table\n",
    "df_gold.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/gold/weather_summary\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .toTable(\"hive_metastore.gold.weather_summary\")\n",
    "\n",
    "df_gold.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1901a50-bfd1-48ad-ba8f-8e22b5373e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Checking whether data is stored in Gold Weather_Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee55e94e-c7f8-4101-a310-2d7a5ce52f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select* from hive_metastore.gold.weather_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb5a4950-0e3e-4a8f-82fa-8d379567f500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Email Notification is sent whenever there is a anomaly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9451a225-a9c6-4526-9ee0-df7b25667f2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_email_alert(alert_data):\n",
    "    sender_email = \"***********\"\n",
    "    receiver_email = \"**********\"\n",
    "    app_password = \"tcxd kowt ppge eujr\"  \n",
    "\n",
    "    subject = \"ðŸš¨ Weather Alert: Anomalies Detected!\"\n",
    "    body = f\"Anomalies detected:\\n\\n{alert_data}\"\n",
    "\n",
    "    msg = MIMEText(body)\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = receiver_email\n",
    "\n",
    "    try:\n",
    "        server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, app_password)\n",
    "        server.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "        server.quit()\n",
    "        print(\"âœ… Alert email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error sending email: {e}\")\n",
    "\n",
    "# Email Alerting System \n",
    "def log_anomalies(batch_df, batch_id):\n",
    "    anomalies = batch_df.filter(col(\"anomaly_count\") > 0)\n",
    "    if anomalies.count() > 0:\n",
    "        anomalies.select(\"start_time\", \"end_time\", \"anomaly_count\").show(truncate=False)\n",
    "        anomalies.write.format(\"delta\").mode(\"append\").saveAsTable(\"hive_metastore.gold.weather_alerts\")\n",
    "\n",
    "        # Convert DataFrame to string for email\n",
    "        alert_text = anomalies.toPandas().to_string(index=False)\n",
    "        send_email_alert(alert_text)\n",
    "    \n",
    "\n",
    "#Checking for new anomalies \n",
    "    if anomalies.count() > 0:\n",
    "        # Check for new anomalies that haven't been processed\n",
    "        existing_alerts = spark.table(\"hive_metastore.gold.weather_alerts\")\n",
    "\n",
    "        print(\"ðŸ“Œ Existing Alerts ðŸ“Œ\")\n",
    "        existing_alerts.show(truncate=False)\n",
    "\n",
    "        new_anomalies = anomalies.alias(\"a\").join(\n",
    "            existing_alerts.alias(\"b\"),\n",
    "            on=[\"start_time\", \"end_time\"],\n",
    "            how=\"left_anti\"\n",
    "        )\n",
    "\n",
    "        print(\"âœ… New Anomalies âœ…\")\n",
    "        new_anomalies.show(truncate=False)\n",
    "\n",
    "        if new_anomalies.count() > 0:\n",
    "            new_anomalies.write.format(\"delta\").mode(\"complete\").saveAsTable(\"hive_metastore.gold.weather_alerts\")\n",
    "\n",
    "            # Convert DataFrame to string for email\n",
    "            alert_text = new_anomalies.toPandas().to_string(index=False)\n",
    "            send_email_alert(alert_text)\n",
    "\n",
    "df_gold.writeStream \\\n",
    "    .foreachBatch(log_anomalies) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "df_gold.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1704710504740928,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Weather Alerts using Real-time Data Processing with Azure Databricks (and Event Hubs)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}